{\n "cells": [\n  {\n   "cell_type": "markdown",\n   "metadata": {},\n   "source": [\n    "# Model Export for MAX78000 Deployment\n",\n    "\n",\n    "This notebook exports the trained and quantized MobileNetV2 model to MAX78000-compatible format.\n",\n    "\n",\n    "## Steps:\n",\n    "1. Load quantized TFLite model\n",\n    "2. Verify model structure\n",\n    "3. Export to ONNX (optional)\n",\n    "4. Prepare for ai8x-synthesis conversion\n",\n    "5. Generate deployment files"\n   ]\n  },\n  {\n   "cell_type": "code",\n   "execution_count": null,\n   "metadata": {},\n   "outputs": [],\n   "source": [\n    "import numpy as np\n",\n    "import tensorflow as tf\n",\n    "import os\n",\n    "import json\n",\n    "from pathlib import Path\n",\n    "\n",\n    "print(f\"TensorFlow version: {tf.__version__}\")"\n   ]\n  },\n  {\n   "cell_type": "markdown",\n   "metadata": {},\n   "source": [\n    "## Configuration"\n   ]\n  },\n  {\n   "cell_type": "code",\n   "execution_count": null,\n   "metadata": {},\n   "outputs": [],\n   "source": [\n    "MODEL_DIR = '../models'\n",\n    "MAX78000_DIR = os.path.join(MODEL_DIR, 'max78000')\n",\n    "os.makedirs(MAX78000_DIR, exist_ok=True)\n",\n    "\n",\n    "TFLITE_MODEL_PATH = os.path.join(MODEL_DIR, 'mobilenetv2_quantized.tflite')\n",\n    "\n",\n    "print(f\"Model directory: {MODEL_DIR}\")\n",\n    "print(f\"MAX78000 output directory: {MAX78000_DIR}\")"\n   ]\n  },\n  {\n   "cell_type": "markdown",\n   "metadata": {},\n   "source": [\n    "## 1. Load and Inspect Quantized Model"\n   ]\n  },\n  {\n   "cell_type": "code",\n   "execution_count": null,\n   "metadata": {},\n   "outputs": [],\n   "source": [\n    "# Load TFLite model\n",\n    "interpreter = tf.lite.Interpreter(model_path=TFLITE_MODEL_PATH)\n",\n    "interpreter.allocate_tensors()\n",\n    "\n",\n    "# Get input and output details\n",\n    "input_details = interpreter.get_input_details()\n",\n    "output_details = interpreter.get_output_details()\n",\n    "\n",\n    "print(\"Input Details:\")\n",\n    "print(json.dumps(input_details[0], indent=2, default=str))\n",\n    "print(\"\nOutput Details:\")\n",\n    "print(json.dumps(output_details[0], indent=2, default=str))\n",\n    "\n",\n    "# Get model size\n",\n    "model_size = os.path.getsize(TFLITE_MODEL_PATH) / 1024\n",\n    "print(f\"\nQuantized model size: {model_size:.2f} KB\")"\n   ]\n  },\n  {\n   "cell_type": "markdown",\n   "metadata": {},\n   "source": [\n    "## 2. Extract Model Metadata"\n   ]\n  },\n  {\n   "cell_type": "code",\n   "execution_count": null,\n   "metadata": {},\n   "outputs": [],\n   "source": [\n    "# Extract metadata for MAX78000 conversion\n",\n    "metadata = {\n",\n    "    'model_name': 'mobilenetv2_breast_cancer',\n",\n    "    'input_shape': input_details[0]['shape'].tolist(),\n",\n    "    'output_shape': output_details[0]['shape'].tolist(),\n",\n    "    'input_dtype': str(input_details[0]['dtype']),\n",\n    "    'output_dtype': str(output_details[0]['dtype']),\n",\n    "    'input_quantization': {\n",\n    "        'scale': float(input_details[0]['quantization'][0]),\n",\n    "        'zero_point': int(input_details[0]['quantization'][1])\n",\n    "    },\n",\n    "    'output_quantization': {\n",\n    "        'scale': float(output_details[0]['quantization'][0]),\n",\n    "        'zero_point': int(output_details[0]['quantization'][1])\n",\n    "    },\n",\n    "    'model_size_kb': model_size,\n",\n    "    'classes': ['healthy', 'cancer']\n",\n    "}\n",\n    "\n",\n    "# Save metadata\n",\n    "metadata_path = os.path.join(MAX78000_DIR, 'model_metadata.json')\n",\n    "with open(metadata_path, 'w') as f:\n",\n    "    json.dump(metadata, f, indent=2)\n",\n    "\n",\n    "print(\"Model Metadata:\")\n",\n    "print(json.dumps(metadata, indent=2))\n",\n    "print(f\"\n✅ Metadata saved to {metadata_path}\")"\n   ]\n  },\n  {\n   "cell_type": "markdown",\n   "metadata": {},\n   "source": [\n    "## 3. Create Deployment Configuration"\n   ]\n  },\n  {\n   "cell_type": "code",\n   "execution_count": null,\n   "metadata": {},\n   "outputs": [],\n   "source": [\n    "# Create deployment configuration for MAX78000\n",\n    "deployment_config = {\n",\n    "    'device': 'MAX78000',\n",\n    "    'model_file': 'mobilenetv2_quantized.tflite',\n",\n    "    'input_size': [96, 96, 3],\n",\n    "    'preprocessing': {\n",\n    "        'resize': [96, 96],\n",\n    "        'normalize': True,\n",\n    "        'mean': [0.0, 0.0, 0.0],\n",\n    "        'std': [255.0, 255.0, 255.0]\n",\n    "    },\n",\n    "    'postprocessing': {\n",\n    "        'activation': 'sigmoid',\n",\n    "        'threshold': 0.5\n",\n    "    },\n",\n    "    'performance': {\n",\n    "        'estimated_inference_time_ms': 100,\n",\n    "        'estimated_power_consumption_mw': 1\n",\n    "    }\n",\n    "}\n",\n    "\n",\n    "config_path = os.path.join(MAX78000_DIR, 'deployment_config.json')\n",\n    "with open(config_path, 'w') as f:\n",\n    "    json.dump(deployment_config, f, indent=2)\n",\n    "\n",\n    "print(\"Deployment Configuration:\")\n",\n    "print(json.dumps(deployment_config, indent=2))\n",\n    "print(f\"\n✅ Configuration saved to {config_path}\")"\n   ]\n  },\n  {\n   "cell_type": "markdown",\n   "metadata": {},\n   "source": [\n    "## 4. Generate C Header File Template"\n   ]\n  },\n  {\n   "cell_type": "code",\n   "execution_count": null,\n   "metadata": {},\n   "outputs": [],\n   "source": [\n    "# Generate C header template for MAX78000\n",\n    "c_header_template = '''/*******************************************************************************\n",\n    "* Copyright (C) 2026 Maxim Integrated Products, Inc., All rights Reserved.\n",\n    "*\n",\n    "* This software is protected by copyright laws of the United States and\n",\n    "* of foreign countries. This material may also be protected by patent laws\n",\n    "* and technology transfer regulations of the United States and of foreign\n",\n    "* countries. This software is furnished under a license agreement and/or a\n",\n    "* nondisclosure agreement and may only be used or reproduced in accordance\n",\n    "* with the terms of those agreements. Dissemination of this information to\n",\n    "* any party or parties not specified in the license agreement and/or\n",\n    "* nondisclosure agreement is expressly prohibited.\n",\n    "*\n",\n    "* Breast Cancer Detection - MobileNetV2 Model\n",\n    "* Generated for MAX78000\n",\n    "*******************************************************************************/\n",\n    "\n",\n    "#ifndef __MODEL_H__\n",\n    "#define __MODEL_H__\n",\n    "\n",\n    "#include <stdint.h>\n",\n    "\n",\n    "// Model configuration\n",\n    "#define MODEL_INPUT_WIDTH   96\n",\n    "#define MODEL_INPUT_HEIGHT  96\n",\n    "#define MODEL_INPUT_CHANNELS 3\n",\n    "#define MODEL_OUTPUT_SIZE   1\n",\n    "\n",\n    "// Class labels\n",\n    "#define CLASS_HEALTHY  0\n",\n    "#define CLASS_CANCER   1\n",\n    "\n",\n    "// Quantization parameters\n",\n    "#define INPUT_SCALE     ''' + str(metadata['input_quantization']['scale']) + '''f\n",\n    "#define INPUT_ZERO_POINT ''' + str(metadata['input_quantization']['zero_point']) + '''\n",\n    "#define OUTPUT_SCALE    ''' + str(metadata['output_quantization']['scale']) + '''f\n",\n    "#define OUTPUT_ZERO_POINT ''' + str(metadata['output_quantization']['zero_point']) + '''\n",\n    "\n",\n    "// Function prototypes\n",\n    "int32_t model_init(void);\n",\n    "int32_t model_run(const uint8_t* input_data, float* output_prob);\n",\n    "const char* get_class_name(int class_id);\n",\n    "\n",\n    "#endif // __MODEL_H__\n",\n    "'''\n",\n    "\n",\n    "header_path = os.path.join(MAX78000_DIR, 'model.h')\n",\n    "with open(header_path, 'w') as f:\n",\n    "    f.write(c_header_template)\n",\n    "\n",\n    "print(\"✅ C header template generated\")\n",\n    "print(f\"Saved to: {header_path}\")"\n   ]\n  },\n  {\n   "cell_type": "markdown",\n   "metadata": {},\n   "source": [\n    "## 5. Generate Example Inference Code"\n   ]\n  },\n  {\n   "cell_type": "code",\n   "execution_count": null,\n   "metadata": {},\n   "outputs": [],\n   "source": [\n    "# Generate example inference code\n",\n    "inference_example = '''/*******************************************************************************\n",\n    "* Breast Cancer Detection - Inference Example\n",\n    "* MAX78000 Implementation\n",\n    "*******************************************************************************/\n",\n    "\n",\n    "#include \"model.h\"\n",\n    "#include <stdio.h>\n",\n    "#include <string.h>\n",\n    "\n",\n    "// Example inference function\n",\n    "void run_inference(const uint8_t* thermal_image) {\n",\n    "    float output_probability;\n",\n    "    int result;\n",\n    "    \n",\n    "    // Initialize model\n",\n    "    result = model_init();\n",\n    "    if (result != 0) {\n",\n    "        printf(\"Model initialization failed!\\n\");\n",\n    "        return;\n",\n    "    }\n",\n    "    \n",\n    "    // Run inference\n",\n    "    result = model_run(thermal_image, &output_probability);\n",\n    "    if (result != 0) {\n",\n    "        printf(\"Inference failed!\\n\");\n",\n    "        return;\n",\n    "    }\n",\n    "    \n",\n    "    // Interpret results\n",\n    "    int predicted_class = (output_probability > 0.5) ? CLASS_CANCER : CLASS_HEALTHY;\n",\n    "    const char* class_name = get_class_name(predicted_class);\n",\n    "    \n",\n    "    printf(\"Prediction: %s (confidence: %.2f%%)\\n", \n",\n    "           class_name, \n",\n    "           (predicted_class == CLASS_CANCER ? output_probability : 1.0 - output_probability) * 100.0);\n",\n    "}\n",\n    "\n",\n    "// Get class name from ID\n",\n    "const char* get_class_name(int class_id) {\n",\n    "    switch(class_id) {\n",\n    "        case CLASS_HEALTHY: return \"Healthy\";\n",\n    "        case CLASS_CANCER:  return \"Cancer Detected\";\n",\n    "        default:            return \"Unknown\";\n",\n    "    }\n",\n    "}\n",\n    "'''

inference_path = os.path.join(MAX78000_DIR, 'inference_example.c')
with open(inference_path, 'w') as f:
    f.write(inference_example)

print("✅ Inference example code generated")
print(f"Saved to: {inference_path}")"\n   ]\n  },\n  {\n   "cell_type": "markdown",\n   "metadata": {},\n   "source": [\n    "## 6. Generate README for MAX78000 Deployment"\n   ]\n  },\n  {\n   "cell_type": "code",\n   "execution_count": null,\n   "metadata": {},\n   "outputs": [],\n   "source": [\n    "readme_content = '''# MAX78000 Deployment Guide\n",\n    "\n",\n    "This directory contains files for deploying the breast cancer detection model on the MAX78000 AI accelerator.\n",\n    "\n",\n    "## Files\n",\n    "\n",\n    "- `mobilenetv2_quantized.tflite` - Quantized TFLite model\n",\n    "- `model_metadata.json` - Model metadata and quantization parameters\n",\n    "- `deployment_config.json` - Deployment configuration\n",\n    "- `model.h` - C header file template\n",\n    "- `inference_example.c` - Example inference code\n",\n    "- `conversion_instructions.md` - Step-by-step conversion guide\n",\n    "\n",\n    "## Conversion Steps\n",\n    "\n",\n    "### Prerequisites\n",\n    "\n",\n    "1. Install MAX78000 SDK and ai8x-synthesis tool:\n",\n    "   ```bash\n",\n    "   git clone https://github.com/MaximIntegrated/ai8x-synthesis.git\n",\n    "   cd ai8x-synthesis\n",\n    "   pip install -r requirements.txt\n",\n    "   ```\n",\n    "\n",\n    "2. Install MAX78000 toolchain\n",\n    "\n",\n    "### Convert TFLite to MAX78000 Format\n",\n    "\n",\n    "```bash\n",\n    "# Step 1: Convert TFLite to ONNX (if needed)\n",\n    "python -m tf2onnx.convert --tflite mobilenetv2_quantized.tflite --output model.onnx\n",\n    "\n",\n    "# Step 2: Convert to MAX78000 using ai8x-synthesis\n",\n    "python ai8xize.py \\
",\n    "  --verbose \\
",\n    "  --test-dir . \\
",\n    "  --prefix mobilenetv2_breast_cancer \\
",\n    "  --checkpoint-file model.pth.tar \\
",\n    "  --config-file networks/mobilenetv2-hwc.yaml \\
",\n    "  --device MAX78000 \\
",\n    "  --compact-data \\
",\n    "  --mexpress\n",\n    "```\n",\n    "\n",\n    "### Deploy to Hardware\n",\n    "\n",\n    "1. Copy generated C files to your MAX78000 project\n",\n    "2. Include model header: `#include \"model.h\"`\n",\n    "3. Call inference function with thermal image data\n",\n    "4. Compile and flash to MAX78000\n",\n    "\n",\n    "## Model Specifications\n",\n    "\n",\n    "- **Input**: 96×96×3 (RGB thermal image)\n",\n    "- **Output**: 1 (probability of cancer)\n",\n    "- **Data Type**: INT8 (quantized)\n",\n    "- **Model Size**: ~50 KB\n",\n    "- **Expected Inference Time**: <100 ms\n",\n    "- **Power Consumption**: <1 mW\n",\n    "\n",\n    "## Preprocessing\n",\n    "\n",\n    "Before feeding data to the model:\n",\n    "1. Resize image to 96×96\n",\n    "2. Convert to RGB (if grayscale)\n",\n    "3. Normalize to [0, 1] range\n",\n    "4. Quantize using input scale and zero-point\n",\n    "\n",\n    "## Postprocessing\n",\n    "\n",\n    "1. Dequantize output using output scale and zero-point\n",\n    "2. Apply sigmoid activation (if not included in model)\n",\n    "3. Threshold at 0.5 for binary classification\n",\n    "\n",\n    "## Support\n",\n    "\n",\n    "For issues with MAX78000 deployment, refer to:\n",\n    "- [MAX78000 Documentation](https://www.analog.com/en/products/max78000.html)\n",\n    "- [ai8x-synthesis GitHub](https://github.com/MaximIntegrated/ai8x-synthesis)\n",\n    "'''\n",\n    "\n",\n    "readme_path = os.path.join(MAX78000_DIR, 'README.md')\n",\n    "with open(readme_path, 'w') as f:\n",\n    "    f.write(readme_content)\n",\n    "\n",\n    "print(\"✅ MAX78000 deployment README generated\")\n",\n    "print(f\"Saved to: {readme_path}\")"\n   ]\n  },\n  {\n   "cell_type": "markdown",\n   "metadata": {},\n   "source": [\n    "## 7. Copy Quantized Model"\n   ]\n  },\n  {\n   "cell_type": "code",\n   "execution_count": null,\n   "metadata": {},\n   "outputs": [],\n   "source": [\n    "import shutil\n",\n    "\n",\n    "# Copy quantized model to MAX78000 directory\n",\n    "dest_model_path = os.path.join(MAX78000_DIR, 'mobilenetv2_quantized.tflite')\n",\n    "shutil.copy(TFLITE_MODEL_PATH, dest_model_path)\n",\n    "\n",\n    "print(f\"✅ Quantized model copied to MAX78000 directory\")\n",\n    "print(f\"Location: {dest_model_path}\")"\n   ]\n  },\n  {\n   "cell_type": "markdown",\n   "metadata": {},\n   "source": [\n    "## 8. Summary"\n   ]\n  },\n  {\n   "cell_type": "code",\n   "execution_count": null,\n   "metadata": {},\n   "outputs": [],\n   "source": [\n    "print(\"\n" + "="*60)\n",\n    "print(\"MODEL EXPORT SUMMARY\")\n",\n    "print("="*60)\n",\n    "print(f\"\nExport Directory: {MAX78000_DIR}\")\n",\n    "print(f\"\nGenerated Files:\")\n",\n    "for file in os.listdir(MAX78000_DIR):\n",\n    "    file_path = os.path.join(MAX78000_DIR, file)\n",\n    "    if os.path.isfile(file_path):\n",\n    "        size = os.path.getsize(file_path)\n",\n    "        print(f\"  - {file} ({size:,} bytes)\")\n",\n    "\n",\n    "print(f"\n{'='*60}")\n",\n    "print("✅ Model export completed!\")\n",\n    "print("\nNext Steps:")\n",\n    "print("1. Install MAX78000 SDK and ai8x-synthesis")\n",\n    "print("2. Follow conversion instructions in max78000/README.md")\n",\n    "print("3. Deploy to MAX78000 hardware when available")\n",\n    "print("="*60)"\n   ]\n  }\n ],\n "metadata": {\n  "kernelspec": {\n   "display_name": "Python 3",\n   "language": "python",\n   "name": "python3"\n  },\n  "language_info": {\n   "codemirror_mode": {\n    "name": "ipython",\n    "version": 3\n   },\n   "file_extension": ".py",\n   "mimetype": "text/x-python",\n   "name": "python",\n   "nbconvert_exporter": "python",\n   "pygments_lexer": "ipython3",\n   "version": "3.8.0"\n  }\n },\n "nbformat": 4,\n "nbformat_minor": 4\n}